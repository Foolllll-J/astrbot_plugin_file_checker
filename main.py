import asyncio
import os
from typing import List, Dict, Optional
import time
import chardet
import subprocess
import re
import pypdfium2 as pdfium

from astrbot.api.event import filter, AstrMessageEvent, MessageChain
from astrbot.api.star import Context, Star, register, StarTools
from astrbot.api import logger
import astrbot.api.message_components as Comp
from astrbot.core.platform.sources.aiocqhttp.aiocqhttp_message_event import AiocqhttpMessageEvent

@register(
    "astrbot_plugin_file_checker",
    "Foolllll",
    "ç¾¤æ–‡ä»¶é¢„è§ˆåŠ©æ‰‹",
    "1.8.0",
    "https://github.com/Foolllll-J/astrbot_plugin_file_checker"
)
class GroupFileCheckerPlugin(Star):
    def __init__(self, context: Context, config: Optional[Dict] = None):
        super().__init__(context)
        self.config = config if config else {}
        self.group_whitelist: List[int] = self.config.get("group_whitelist", [])
        self.group_whitelist = [int(gid) for gid in self.group_whitelist]
        self.notify_on_success: bool = self.config.get("notify_on_success", True)
        self.pre_check_delay_seconds: int = self.config.get("pre_check_delay_seconds", 5)
        self.check_delay_seconds: int = self.config.get("check_delay_seconds", 300)
        self.preview_length: int = self.config.get("preview_length", 500)
        self.enable_duplicate_check: bool = self.config.get("enable_duplicate_check", False)
        self.enable_zip_preview: bool = self.config.get("enable_zip_preview", True)
        self.zip_extraction_size_limit_mb: int = self.config.get("zip_extraction_size_limit_mb", 100)
        self.default_zip_password: str = self.config.get("default_zip_password", "")
        
        # 7za æ”¯æŒçš„å‹ç¼©æ ¼å¼ï¼ˆä¸åŒ…æ‹¬ RAR5ï¼‰
        self.supported_archive_formats = (
            '.zip', '.7z', '.tar', '.gz', '.bz2', '.xz',
            '.tar.gz', '.tgz', '.tar.bz2', '.tbz2', '.tar.xz', '.txz',
            '.iso', '.wim', '.rar'
        )
        
        # æ”¯æŒæ–‡æœ¬é¢„è§ˆçš„æ–‡ä»¶æ ¼å¼
        self.supported_text_formats = (
            # æ–‡æ¡£ç±»
            '.txt', '.md', '.log',
            # é…ç½®ç±»
            '.json', '.xml', '.yaml', '.yml', '.ini', '.conf', '.cfg', '.toml',
            # ä»£ç ç±»
            '.py', '.js', '.java', '.c', '.cpp', '.h', '.go', '.rs', '.php', '.rb', '.sh', '.bash',
            '.html', '.htm', '.css', '.jsx', '.tsx', '.ts', '.vue', '.sql',
            # æ•°æ®ç±»
            '.csv', '.properties', '.env'
        )
        repack_extensions_str: str = self.config.get("repack_file_extensions", "")
        self.repack_file_extensions: List[str] = [ext.strip().lower() for ext in repack_extensions_str.split(",") if ext.strip()]
        self.repack_zip_password: str = self.config.get("repack_zip_password", "")
        self.file_size_threshold_mb: int = self.config.get("file_size_threshold_mb", 100)
        
        # åª’ä½“è½¬æ¢é…ç½®
        self.auto_convert_video_threshold_mb: int = self.config.get("auto_convert_video_threshold_mb", 0)
        self.enable_auto_convert_image = self.config.get("enable_auto_convert_image", False)
        self.pdf_preview_pages = self.config.get("pdf_preview_pages", 0)
        self.image_convert_max_size_mb = 15  # å›¾ç‰‡è½¬æ¢å¤§å°é™åˆ¶ï¼Œè¶…è¿‡15MBä¼šä¸ç¨³å®š
        
        self.temp_dir = os.path.join(StarTools.get_data_dir("astrbot_plugin_file_checker"), "temp")
        os.makedirs(self.temp_dir, exist_ok=True)
        
        self.download_semaphore = asyncio.Semaphore(5)
        logger.info("æ’ä»¶ [ç¾¤æ–‡ä»¶å¤±æ•ˆæ£€æŸ¥] å·²åŠ è½½ã€‚")

    def _find_file_component(self, event: AstrMessageEvent) -> Optional[Comp.File]:
        for segment in event.get_messages():
            if isinstance(segment, Comp.File):
                return segment
        return None

    def _fix_zip_filename(self, filename: str) -> str:
        try:
            return filename.encode('cp437').decode('gbk')
        except (UnicodeEncodeError, UnicodeDecodeError):
            return filename
    
    def _is_video_file(self, filename: str) -> bool:
        """æ£€æµ‹æ–‡ä»¶æ˜¯å¦ä¸ºè§†é¢‘æ ¼å¼ï¼ˆä»…æ”¯æŒ mp4ï¼‰"""
        file_ext = os.path.splitext(filename)[1].lower()
        return file_ext == '.mp4'
    
    def _is_image_file(self, filename: str) -> bool:
        """æ£€æµ‹æ–‡ä»¶æ˜¯å¦ä¸ºå›¾ç‰‡æ ¼å¼ï¼ˆæ”¯æŒ jpg/jpeg/png/gif/webpï¼‰"""
        file_ext = os.path.splitext(filename)[1].lower()
        return file_ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']
    
    def _is_pdf_file(self, filename: str) -> bool:
        """æ£€æµ‹æ–‡ä»¶æ˜¯å¦ä¸º PDF æ ¼å¼"""
        file_ext = os.path.splitext(filename)[1].lower()
        return file_ext == '.pdf'
    
    async def _delete_group_file(self, event: AstrMessageEvent, file_id: str, file_name: str) -> bool:
        """åˆ é™¤ç¾¤æ–‡ä»¶"""
        group_id = int(event.get_group_id())
        try:
            client = event.bot
            delete_result = await client.api.call_action('delete_group_file', group_id=group_id, file_id=file_id)
            
            if delete_result and delete_result.get('transGroupFileResult', {}).get('result', {}).get('retCode') == 0:
                logger.info(f"[{group_id}] âœ… æˆåŠŸåˆ é™¤ç¾¤æ–‡ä»¶: {file_name}")
                return True
            else:
                logger.warning(f"[{group_id}] âš ï¸ åˆ é™¤ç¾¤æ–‡ä»¶å¤±è´¥: {file_name}")
                return False
        except Exception as e:
            logger.error(f"[{group_id}] âŒ åˆ é™¤ç¾¤æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            return False
    
    async def _convert_file_to_media(self, event: AstrMessageEvent, file_name: str, file_id: str, file_component: Comp.File, file_size: int, media_type: str):
        """
        å°†æ–‡ä»¶è½¬æ¢ä¸ºåª’ä½“å½¢å¼å‘é€ï¼ˆæ”¯æŒè§†é¢‘å’Œå›¾ç‰‡ï¼‰
        
        Args:
            media_type: 'video' æˆ– 'image'
        """
        group_id = int(event.get_group_id())
        local_file_path = None
        
        try:
            media_name = "è§†é¢‘" if media_type == "video" else "å›¾ç‰‡"
            emoji = "ğŸ¬" if media_type == "video" else "ğŸ–¼ï¸"
            logger.info(f"[{group_id}] {emoji} å¼€å§‹{media_name}è½¬æ¢æµç¨‹: {file_name}")
            
            async with self.download_semaphore:
                local_file_path = await file_component.get_file()
            
            if not local_file_path or not os.path.exists(local_file_path):
                logger.error(f"[{group_id}] âŒ ä¸‹è½½{media_name}æ–‡ä»¶å¤±è´¥")
                return
            
            file_size_mb = file_size / (1024 * 1024)
            absolute_path = os.path.abspath(local_file_path)
            
            logger.info(f"[{group_id}] ğŸ“¤ å‡†å¤‡ä»¥{media_name}å½¢å¼å‘é€æ–‡ä»¶ ({file_size_mb:.2f} MB): {absolute_path}")
            
            if media_type == "video":
                media_message = [Comp.Video(file=f"file:///{absolute_path}")]
            else:  # image
                media_message = [Comp.Image.fromFileSystem(absolute_path)]
            
            yield event.chain_result(media_message)
            
            logger.info(f"[{group_id}] âœ… {media_name}å‘é€æˆåŠŸï¼Œå°†åœ¨ 30 åˆ†é’Ÿååˆ é™¤ç¾¤æ–‡ä»¶å’Œæœ¬åœ°ç¼“å­˜")
            
            # 30åˆ†é’Ÿååˆ é™¤ç¾¤æ–‡ä»¶å’Œæœ¬åœ°ç¼“å­˜
            delete_delay = 1800  # 30åˆ†é’Ÿ
            asyncio.create_task(self._delayed_cleanup(event, file_name, local_file_path, delete_delay))
            
            return  # è½¬æ¢æˆåŠŸ
            
        except Exception as e:
            media_name = "è§†é¢‘" if media_type == "video" else "å›¾ç‰‡"
            logger.error(f"[{group_id}] âŒ {media_name}å‘é€å¤±è´¥: {e}", exc_info=True)
            if local_file_path and os.path.exists(local_file_path):
                try:
                    os.remove(local_file_path)
                    logger.info(f"[{group_id}] ğŸ—‘ï¸ å·²æ¸…ç†ä¸‹è½½å¤±è´¥çš„æœ¬åœ°{media_name}ç¼“å­˜")
                except OSError:
                    pass
            return  # å‘é€å¤±è´¥
    
    async def _delayed_cleanup(self, event: AstrMessageEvent, file_name: str, local_path: str, delay: int):
        """å»¶è¿Ÿæ¸…ç†ç¾¤æ–‡ä»¶å’Œæœ¬åœ°æ–‡ä»¶"""
        await asyncio.sleep(delay)
        
        group_id = int(event.get_group_id())
        logger.info(f"[{group_id}] å¼€å§‹å»¶è¿Ÿæ¸…ç†è§†é¢‘æ–‡ä»¶: {file_name}")
        
        # é€šè¿‡æ–‡ä»¶åæŸ¥è¯¢æœ€æ–°çš„ file_id
        file_id = await self._search_file_id_by_name(event, file_name)
        
        if file_id:
            await self._delete_group_file(event, file_id, file_name)
        else:
            logger.error(f"[{group_id}] âŒ æ— æ³•æŸ¥è¯¢åˆ°æ–‡ä»¶IDï¼Œå¯èƒ½æ–‡ä»¶å·²è¢«åˆ é™¤æˆ–ç§»åŠ¨")
        
        # åˆ é™¤æœ¬åœ°æ–‡ä»¶
        if local_path and os.path.exists(local_path):
            try:
                os.remove(local_path)
                logger.info(f"[{group_id}] ğŸ—‘ï¸ å·²åˆ é™¤æœ¬åœ°è§†é¢‘ç¼“å­˜: {os.path.basename(local_path)}")
            except OSError as e:
                logger.warning(f"[{group_id}] âš ï¸ åˆ é™¤æœ¬åœ°è§†é¢‘ç¼“å­˜å¤±è´¥: {e}")
    
    async def _search_file_id_by_name(self, event: AstrMessageEvent, file_name: str) -> Optional[str]:
        group_id = int(event.get_group_id())
        
        try:
            client = event.bot
            file_list = await client.api.call_action('get_group_root_files', group_id=group_id)
            
            if not isinstance(file_list, dict) or 'files' not in file_list:
                logger.warning("get_group_root_files APIè°ƒç”¨è¿”å›äº†æ„æ–™ä¹‹å¤–çš„æ ¼å¼ã€‚")
                return None
            
            for file_info in file_list.get('files', []):
                if file_info.get('file_name') == file_name:
                    file_id = file_info.get('file_id')
                    logger.debug(f"[{group_id}] æŸ¥è¯¢åˆ°æ–‡ä»¶ '{file_name}' çš„ file_id: {file_id}")
                    return file_id
            
            logger.warning(f"[{group_id}] æœªæ‰¾åˆ°æ–‡ä»¶ '{file_name}'")
            return None
        except Exception as e:
            logger.error(f"[{group_id}] é€šè¿‡æ–‡ä»¶åæœç´¢æ–‡ä»¶IDæ—¶å‡ºé”™: {e}", exc_info=True)
            return None

    async def _check_if_file_exists_by_size(self, event: AstrMessageEvent, file_name: str, file_size: int, upload_time: int) -> List[Dict]:
        group_id = int(event.get_group_id())
        
        client = event.bot
        all_files_dict = {}
        folders_to_scan = [{'folder_id': '/', 'folder_name': 'æ ¹ç›®å½•'}]
        
        while folders_to_scan:
            current_folder = folders_to_scan.pop(0)
            current_folder_id = current_folder['folder_id']
            current_folder_name = current_folder['folder_name']
            
            try:
                if current_folder_id == '/':
                    result = await client.api.call_action('get_group_root_files', group_id=group_id)
                else:
                    result = await client.api.call_action('get_group_files_by_folder', group_id=group_id, folder_id=current_folder_id, file_count=1000)

                if not isinstance(result, dict):
                    logger.warning(f"[{group_id}] APIè¿”å›äº†æ„æ–™ä¹‹å¤–çš„æ ¼å¼ã€‚")
                    continue
                
                for file_info in result.get('files', []):
                    file_info['parent_folder_name'] = current_folder_name
                    all_files_dict[file_info.get('file_id')] = file_info
                
                for folder_info in result.get('folders', []):
                    folders_to_scan.append(folder_info)

            except Exception as e:
                logger.error(f"[{group_id}] éå†æ–‡ä»¶å¤¹ '{current_folder['folder_name']}' æ—¶å‡ºé”™: {e}", exc_info=True)
        
        logger.debug(f"[{group_id}] éå†å®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_files_dict)} ä¸ªæ–‡ä»¶ã€‚")
        
        possible_duplicates = []
        for file_info in all_files_dict.values():
            if file_info.get('file_size') == file_size:
                possible_duplicates.append(file_info)

        logger.debug(f"[{group_id}] å…±æ‰¾åˆ° {len(possible_duplicates)} ä¸ªå¤§å°åŒ¹é…çš„å€™é€‰é¡¹ã€‚")
        
        existing_files = []
        removed_files = []
        
        for f in possible_duplicates:
            file_modify_time = f.get('modify_time')
            
            if file_modify_time is not None:
                file_time_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(file_modify_time))
                
                if abs(file_modify_time - upload_time) <= 2:
                    removed_files.append(f)
                else:
                    existing_files.append(f)
            else:
                existing_files.append(f)

        if removed_files:
            logger.debug(f"[{group_id}] å·²ä»å€™é€‰é¡¹ä¸­æ’é™¤è‡ªèº«æ–‡ä»¶ï¼Œå…± {len(removed_files)} ä¸ªã€‚")
        
        if existing_files:
            logger.info(f"[{group_id}] æœ€ç»ˆç¡®è®¤ {len(existing_files)} ä¸ªçœŸæ­£çš„é‡å¤æ–‡ä»¶ã€‚")
            for f in existing_files:
                modify_time_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(f.get('modify_time', 0)))
                logger.info(
                    f"  â†³ æ–‡ä»¶å: '{f.get('file_name', 'æœªçŸ¥')}'\n"
                    f"    æ–‡ä»¶ID: {f.get('file_id', 'æœªçŸ¥')}\n"
                    f"    å¤§å°: {f.get('file_size', 'æœªçŸ¥')}å­—èŠ‚\n"
                    f"    ä¸Šä¼ è€…: {f.get('uploader_name', 'æœªçŸ¥')}\n"
                    f"    ä¿®æ”¹æ—¶é—´: {modify_time_str}\n"
                    f"    æ‰€å±æ–‡ä»¶å¤¹: {f.get('parent_folder_name', 'æ ¹ç›®å½•')}"
                )
        else:
            logger.info(f"[{group_id}] æœªæ‰¾åˆ°çœŸæ­£çš„é‡å¤æ–‡ä»¶ã€‚")
        
        return existing_files
    
    @filter.event_message_type(filter.EventMessageType.GROUP_MESSAGE, priority=2)
    async def on_group_message(self, event: AstrMessageEvent, *args, **kwargs):
        """å¤„ç†ç¾¤æ¶ˆæ¯äº‹ä»¶"""
        group_id = int(event.get_group_id())
        if self.group_whitelist and group_id not in self.group_whitelist:
            return
        
        try:
            raw_event_data = event.message_obj.raw_message
            message_list = raw_event_data.get("message")
            if not isinstance(message_list, list):
                return
            for segment_dict in message_list:
                if isinstance(segment_dict, dict) and segment_dict.get("type") == "file":
                    data_dict = segment_dict.get("data", {})
                    file_name = data_dict.get("file")
                    file_id = data_dict.get("file_id")
                    file_size = data_dict.get("file_size")

                    if isinstance(file_size, str):
                        try:
                            file_size = int(file_size)
                        except ValueError:
                            logger.error(f"æ— æ³•å°†æ–‡ä»¶å¤§å° '{file_size}' è½¬æ¢ä¸ºæ•´æ•°ï¼Œå·²è·³è¿‡é‡å¤æ€§æ£€æŸ¥ã€‚")
                            file_size = None

                    if file_name and file_id:
                        if file_size is not None and self.file_size_threshold_mb > 0:
                            file_size_mb = file_size / (1024 * 1024)
                            if file_size_mb > self.file_size_threshold_mb:
                                logger.debug(f"[{group_id}] æ–‡ä»¶ '{file_name}' å¤§å° ({file_size_mb:.2f} MB) è¶…è¿‡å¤„ç†é˜ˆå€¼ ({self.file_size_threshold_mb} MB)ï¼Œè·³è¿‡æ‰€æœ‰å¤„ç†ã€‚")
                                return
                        logger.debug(f"æˆåŠŸè§£æ: æ–‡ä»¶å='{file_name}', ID='{file_id}'")
                        file_component = self._find_file_component(event)
                        if not file_component:
                            logger.error("è‡´å‘½é”™è¯¯ï¼šæ— æ³•åœ¨ç»„ä»¶ä¸­æ‰¾åˆ°å¯¹åº”çš„Fileå¯¹è±¡ï¼")
                            return
                        
                        if self.enable_duplicate_check and file_size is not None:
                            upload_time = raw_event_data.get("time", int(time.time()))
                            logger.debug(f"[{group_id}] æ–°ä¸Šä¼ æ–‡ä»¶æ—¶é—´æˆ³: {upload_time} ({time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(upload_time))})")

                            existing_files = await self._check_if_file_exists_by_size(event, file_name, file_size, upload_time)
                            if existing_files:
                                if len(existing_files) == 1:
                                    existing_file = existing_files[0]
                                    reply_text = (
                                        f"ğŸ’¡ æé†’ï¼šæ‚¨å‘é€çš„æ–‡ä»¶ã€Œ{file_name}ã€å¯èƒ½ä¸ç¾¤æ–‡ä»¶ä¸­çš„ã€Œ{existing_file.get('file_name', 'æœªçŸ¥æ–‡ä»¶å')}ã€é‡å¤ã€‚\n"
                                        f"  â†³ ä¸Šä¼ è€…: {existing_file.get('uploader_name', 'æœªçŸ¥')}\n"
                                        f"  â†³ ä¿®æ”¹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(existing_file.get('modify_time', 0)))}\n"
                                        f"  â†³ æ‰€å±æ–‡ä»¶å¤¹: {existing_file.get('parent_folder_name', 'æ ¹ç›®å½•')}"
                                    )
                                    yield event.chain_result([Comp.Reply(id=event.message_obj.message_id), Comp.Plain(reply_text)])
                                else:
                                    reply_text = f"ğŸ’¡ æé†’ï¼šæ‚¨å‘é€çš„æ–‡ä»¶ã€Œ{file_name}ã€å¯èƒ½ä¸ç¾¤æ–‡ä»¶ä¸­ä»¥ä¸‹ {len(existing_files)} ä¸ªæ–‡ä»¶é‡å¤ï¼š\n"
                                    for idx, file_info in enumerate(existing_files, 1):
                                        reply_text += (
                                            f"\n{idx}. {file_info.get('file_name', 'æœªçŸ¥æ–‡ä»¶å')}\n"
                                            f"    â†³ ä¸Šä¼ è€…: {file_info.get('uploader_name', 'æœªçŸ¥')}\n"
                                            f"    â†³ ä¿®æ”¹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(file_info.get('modify_time', 0)))}\n"
                                            f"    â†³ æ‰€å±æ–‡ä»¶å¤¹: {file_info.get('parent_folder_name', 'æ ¹ç›®å½•')}"
                                        )
                                    yield event.chain_result([Comp.Reply(id=event.message_obj.message_id), Comp.Plain(reply_text)])
                                break

                        if self.auto_convert_video_threshold_mb > 0 and file_size is not None:
                            if self._is_video_file(file_name):
                                file_size_mb = file_size / (1024 * 1024)
                                if file_size_mb > self.auto_convert_video_threshold_mb:
                                    logger.debug(f"[{group_id}] è§†é¢‘æ–‡ä»¶ '{file_name}' ({file_size_mb:.2f} MB) è¶…è¿‡è½¬æ¢é˜ˆå€¼ ({self.auto_convert_video_threshold_mb} MB)ï¼Œè·³è¿‡è‡ªåŠ¨è½¬æ¢")

                        async for result in self._handle_file_check_flow(event, file_name, file_id, file_component, file_size):
                            yield result
                        break
        except Exception as e:
            logger.error(f"ã€åŸå§‹æ–¹å¼ã€‘å¤„ç†æ¶ˆæ¯æ—¶å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}", exc_info=True)

    async def _repack_and_send_file(self, event: AstrMessageEvent, original_filename: str, file_component: Comp.File):
        base_name = os.path.basename(original_filename)
        if re.search(r'[\\/|*<>;"\x00-\x1F\x7F]', base_name):
            logger.error(f"æ–‡ä»¶å '{original_filename}' åŒ…å«éå®‰å…¨å­—ç¬¦ï¼Œå·²è·³è¿‡é‡æ–°æ‰“åŒ…ã€‚")
            yield event.chain_result([Comp.Reply(id=event.message_obj.message_id), Comp.Plain("âŒ æ–‡ä»¶ååŒ…å«ä¸å®‰å…¨å­—ç¬¦ï¼Œå·²è·³è¿‡é‡æ–°æ‰“åŒ…ã€‚")])
            return
        
        repacked_file_path = None
        original_txt_path = None
        renamed_txt_path = None
        try:
            logger.info(f"å¼€å§‹ä¸ºå¤±æ•ˆæ–‡ä»¶ {original_filename} è¿›è¡Œé‡æ–°æ‰“åŒ…...")
            
            original_txt_path = await file_component.get_file()
            
            renamed_txt_path = os.path.join(self.temp_dir, original_filename)
            if os.path.exists(renamed_txt_path):
                os.remove(renamed_txt_path)
            os.rename(original_txt_path, renamed_txt_path)

            base_name = os.path.splitext(original_filename)[0]
            new_zip_name = f"{base_name}.zip"
            repacked_file_path = os.path.join(self.temp_dir, f"{int(time.time())}_{new_zip_name}")

            command = ['zip', '-j', repacked_file_path, renamed_txt_path]
            if self.repack_zip_password:
                command.extend(['-P', self.repack_zip_password])

            logger.debug(f"æ­£åœ¨æ‰§è¡Œæ‰“åŒ…å‘½ä»¤: {' '.join(command)}")
            process = await asyncio.create_subprocess_exec(
                *command,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            stdout, stderr = await process.communicate()
            
            if process.returncode != 0:
                error_message = stderr.decode('utf-8')
                logger.error(f"ä½¿ç”¨ zip å‘½ä»¤æ‰“åŒ…æ–‡ä»¶æ—¶å‡ºé”™: {error_message}")
                yield event.chain_result([Comp.Reply(id=event.message_obj.message_id), Comp.Plain(f"âŒ é‡æ–°æ‰“åŒ…å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š\n{error_message}")])
                return
            
            logger.info(f"æ–‡ä»¶å·²é‡æ–°æ‰“åŒ…è‡³ {repacked_file_path}ï¼Œå‡†å¤‡å‘é€...")
            
            reply_text = "å·²ä¸ºæ‚¨é‡æ–°æ‰“åŒ…ä¸ºZIPæ–‡ä»¶å‘é€ï¼š"
            file_component_to_send = Comp.File(file=repacked_file_path, name=new_zip_name)
            
            yield event.chain_result([Comp.Reply(id=event.message_obj.message_id), Comp.Plain(reply_text)])
            
            yield event.chain_result([file_component_to_send])
            
            await asyncio.sleep(2)
            
            new_file_id = await self._search_file_id_by_name(event, new_zip_name)
            
            if new_file_id:
                logger.info(f"æ–°æ–‡ä»¶å‘é€æˆåŠŸï¼ŒIDä¸º {new_file_id}ï¼Œå·²åŠ å…¥å»¶æ—¶å¤æ ¸é˜Ÿåˆ—ã€‚")
                asyncio.create_task(self._task_delayed_recheck(event, new_zip_name, new_file_id, None, None))
            else:
                logger.error("æœªèƒ½è·å–æ–°æ–‡ä»¶çš„IDï¼Œæ— æ³•è¿›è¡Œå»¶æ—¶å¤æ ¸ã€‚")
            
        except FileNotFoundError:
            logger.error("é‡æ–°æ‰“åŒ…å¤±è´¥ï¼šå®¹å™¨å†…æœªæ‰¾åˆ° zip å‘½ä»¤ã€‚è¯·å®‰è£… zipã€‚")
            yield event.chain_result([Comp.Reply(id=event.message_obj.message_id), Comp.Plain("âŒ é‡æ–°æ‰“åŒ…å¤±è´¥ã€‚å®¹å™¨å†…æœªæ‰¾åˆ° zip å‘½ä»¤ï¼Œè¯·è”ç³»ç®¡ç†å‘˜å®‰è£…ã€‚")])
        except Exception as e:
            logger.error(f"é‡æ–°æ‰“åŒ…å¹¶å‘é€æ–‡ä»¶æ—¶å‡ºé”™: {e}", exc_info=True)
            yield event.chain_result([Comp.Reply(id=event.message_obj.message_id), Comp.Plain("âŒ é‡æ–°æ‰“åŒ…å¹¶å‘é€æ–‡ä»¶å¤±è´¥ã€‚")])
        finally:
            if repacked_file_path and os.path.exists(repacked_file_path):
                async def cleanup_file(path: str):
                    await asyncio.sleep(10)
                    try:
                        os.remove(path)
                        logger.info(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {path}")
                    except OSError as e:
                        logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {path} å¤±è´¥: {e}")
                asyncio.create_task(cleanup_file(repacked_file_path))

            if renamed_txt_path and os.path.exists(renamed_txt_path):
                try:
                    os.remove(renamed_txt_path)
                    logger.info(f"å·²æ¸…ç†é‡å‘½ååçš„ä¸´æ—¶æ–‡ä»¶: {renamed_txt_path}")
                except OSError as e:
                    logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {renamed_txt_path} å¤±è´¥: {e}")
    
    async def _handle_file_check_flow(self, event: AstrMessageEvent, file_name: str, file_id: str, file_component: Comp.File, file_size: Optional[int] = None):
        group_id = int(event.get_group_id())
        
        sender_id = event.get_sender_id()
        self_id = event.get_self_id()
        if sender_id == self_id:
            logger.info(f"[{group_id}] æœºå™¨äººå‘é€çš„æ–‡ä»¶ï¼Œç›´æ¥è·³è¿‡å¤„ç†ã€‚")
            return
        
        await asyncio.sleep(self.pre_check_delay_seconds)
        logger.info(f"[{group_id}] [é˜¶æ®µä¸€] å¼€å§‹å³æ—¶æ£€æŸ¥: '{file_name}'")

        is_gfs_valid = await self._check_validity_via_gfs(event, file_id)

        preview_text, preview_extra_info = await self._get_preview_for_file(file_name, file_component, file_size)

        # PDF é¢„è§ˆç”Ÿæˆ (æ— è®ºæœ‰æ•ˆæ€§ï¼Œå’Œæ–‡æœ¬é¢„è§ˆä¿æŒä¸€è‡´)
        pdf_preview_nodes = []
        if self.pdf_preview_pages > 0 and self._is_pdf_file(file_name):
            logger.info(f"[{group_id}] ğŸ“„ å°è¯•ç”Ÿæˆ PDF é¢„è§ˆ ({self.pdf_preview_pages} é¡µ)")
            local_pdf_path = None
            try:
                async with self.download_semaphore:
                    local_pdf_path = await file_component.get_file()
                
                if local_pdf_path and os.path.exists(local_pdf_path):
                    image_paths = await self._get_pdf_preview(local_pdf_path)
                    if image_paths:
                        sender_id = event.get_self_id()
                        for img_path in image_paths:
                            pdf_preview_nodes.append(Comp.Node(uin=sender_id, name="PDF é¢„è§ˆ", content=[Comp.Image.fromFileSystem(img_path)]))
                        
                        # å»¶è¿Ÿæ¸…ç†å›¾ç‰‡
                        async def cleanup_images(paths):
                            await asyncio.sleep(60)
                            for p in paths:
                                try:
                                    if os.path.exists(p): os.remove(p)
                                except: pass
                        asyncio.create_task(cleanup_images(image_paths))
            except Exception as e:
                logger.error(f"[{group_id}] PDF é¢„è§ˆå¤„ç†å‡ºé”™: {e}", exc_info=True)
            finally:
                if local_pdf_path and os.path.exists(local_pdf_path):
                    try: os.remove(local_pdf_path)
                    except: pass

        if is_gfs_valid:
            # æ–‡ä»¶æœ‰æ•ˆï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦åª’ä½“è½¬æ¢ï¼ˆè§†é¢‘æˆ–å›¾ç‰‡ï¼‰
            should_convert_video = (
                self.auto_convert_video_threshold_mb > 0 
                and file_size is not None 
                and self._is_video_file(file_name)
                and (file_size / (1024 * 1024)) <= self.auto_convert_video_threshold_mb
            )
            
            should_convert_image = (
                self.enable_auto_convert_image
                and file_size is not None 
                and self._is_image_file(file_name)
                and (file_size / (1024 * 1024)) <= self.image_convert_max_size_mb
            )
            
            if should_convert_video:
                logger.info(f"[{group_id}] ğŸ¬ æ–‡ä»¶æœ‰æ•ˆï¼Œç¬¦åˆè§†é¢‘è½¬æ¢æ¡ä»¶ï¼Œå°è¯•è½¬æ¢")
                # å°è¯•è½¬æ¢ï¼Œä¸ç®¡æˆåŠŸä¸å¦éƒ½ç»§ç»­æ­£å¸¸æµç¨‹
                async for result in self._convert_file_to_media(event, file_name, file_id, file_component, file_size, "video"):
                    yield result
            elif should_convert_image:
                logger.info(f"[{group_id}] ğŸ–¼ï¸ æ–‡ä»¶æœ‰æ•ˆï¼Œç¬¦åˆå›¾ç‰‡è½¬æ¢æ¡ä»¶ï¼Œå°è¯•è½¬æ¢")
                # å°è¯•è½¬æ¢ï¼Œä¸ç®¡æˆåŠŸä¸å¦éƒ½ç»§ç»­æ­£å¸¸æµç¨‹
                async for result in self._convert_file_to_media(event, file_name, file_id, file_component, file_size, "image"):
                    yield result
            
            if self.notify_on_success:
                success_message = f"âœ… æ‚¨å‘é€çš„æ–‡ä»¶ã€Œ{file_name}ã€åˆæ­¥æ£€æŸ¥æœ‰æ•ˆã€‚"
                if preview_text:
                    # æ–‡ä»¶ç»“æ„åˆ—è¡¨ä¸æˆªæ–­ï¼Œæ™®é€šæ–‡æœ¬é¢„è§ˆæ‰æˆªæ–­
                    is_file_structure = preview_extra_info == "æ–‡ä»¶ç»“æ„"
                    if is_file_structure:
                        preview_text_short = preview_text
                    else:
                        preview_text_short = preview_text[:self.preview_length]
                    
                    success_message += f"\n{preview_extra_info}ï¼Œä»¥ä¸‹æ˜¯é¢„è§ˆï¼š\n{preview_text_short}"
                    if not is_file_structure and len(preview_text) > self.preview_length:
                        success_message += "..."
                
                if pdf_preview_nodes:
                    # å°†æ–‡å­—é€šçŸ¥ä½œä¸ºåˆå¹¶è½¬å‘çš„ç¬¬ä¸€æ¡è®°å½•
                    success_message += f"\nğŸ“„ PDF é¢„è§ˆå›¾å¦‚ä¸‹ï¼š"
                    sender_id = event.get_self_id()
                    pdf_preview_nodes.insert(0, Comp.Node(uin=sender_id, name="PDF é¢„è§ˆ", content=[Comp.Plain(success_message)]))
                    yield event.chain_result([Comp.Nodes(nodes=pdf_preview_nodes)])
                    logger.info(f"[{group_id}] âœ… PDF é¢„è§ˆå·²å‘é€ ({len(pdf_preview_nodes)-1} é¡µï¼ŒåŒ…å«æ–‡å­—é€šçŸ¥)")
                else:
                    yield event.chain_result([Comp.Reply(id=event.message_obj.message_id), Comp.Plain(success_message)])
            elif pdf_preview_nodes:
                # å¦‚æœæ²¡å¼€å¯æˆåŠŸé€šçŸ¥ä½†æœ‰ PDF é¢„è§ˆ
                yield event.chain_result([Comp.Nodes(nodes=pdf_preview_nodes)])
                logger.info(f"[{group_id}] âœ… PDF é¢„è§ˆå·²å‘é€ ({len(pdf_preview_nodes)} é¡µ)")

            logger.info(f"[{group_id}] åˆæ­¥æ£€æŸ¥é€šè¿‡ï¼Œå·²åŠ å…¥å»¶æ—¶å¤æ ¸é˜Ÿåˆ—ã€‚")
            asyncio.create_task(self._task_delayed_recheck(event, file_name, file_id, file_component, preview_text))
        else:
            logger.error(f"âŒ [{group_id}] [é˜¶æ®µä¸€] æ–‡ä»¶ '{file_name}' å³æ—¶æ£€æŸ¥å·²å¤±æ•ˆ!")
            try:
                failure_message = f"âš ï¸ æ‚¨å‘é€çš„æ–‡ä»¶ã€Œ{file_name}ã€å·²å¤±æ•ˆã€‚"
                if preview_text:
                    # æ–‡ä»¶ç»“æ„åˆ—è¡¨ä¸æˆªæ–­ï¼Œæ™®é€šæ–‡æœ¬é¢„è§ˆæ‰æˆªæ–­
                    is_file_structure = preview_extra_info == "æ–‡ä»¶ç»“æ„"
                    if is_file_structure:
                        preview_text_short = preview_text
                    else:
                        preview_text_short = preview_text[:self.preview_length]
                    
                    failure_message += f"\n{preview_extra_info}ï¼Œä»¥ä¸‹æ˜¯é¢„è§ˆï¼š\n{preview_text_short}"
                    if not is_file_structure and len(preview_text) > self.preview_length:
                        failure_message += "..."
                
                if pdf_preview_nodes:
                    # å°†æ–‡å­—é€šçŸ¥ä½œä¸ºåˆå¹¶è½¬å‘çš„ç¬¬ä¸€æ¡è®°å½•
                    failure_message += f"\nğŸ“„ PDF é¢„è§ˆå›¾å¦‚ä¸‹ï¼š"
                    sender_id = event.get_self_id()
                    pdf_preview_nodes.insert(0, Comp.Node(uin=sender_id, name="PDF é¢„è§ˆ", content=[Comp.Plain(failure_message)]))
                    yield event.chain_result([Comp.Nodes(nodes=pdf_preview_nodes)])
                    logger.info(f"[{group_id}] âœ… PDF é¢„è§ˆå·²å‘é€ ({len(pdf_preview_nodes)-1} é¡µï¼ŒåŒ…å«æ–‡å­—é€šçŸ¥)")
                else:
                    yield event.chain_result([Comp.Reply(id=event.message_obj.message_id), Comp.Plain(failure_message)])

                if self.repack_file_extensions:
                    file_ext = os.path.splitext(file_name)[1].lower().lstrip('.')
                    if file_ext in self.repack_file_extensions:
                        logger.info(f"æ–‡ä»¶å³æ—¶æ£€æŸ¥å¤±æ•ˆï¼Œè§¦å‘é‡æ–°æ‰“åŒ…ä»»åŠ¡ (æ–‡ä»¶ç±»å‹: {file_ext})...")
                        async for result in self._repack_and_send_file(event, file_name, file_component):
                            yield result
                        # è¡¥æ¡£ååˆ é™¤å·²å¤±æ•ˆçš„åŸæ–‡ä»¶
                        logger.info(f"[{group_id}] è¡¥æ¡£å®Œæˆï¼Œåˆ é™¤å·²å¤±æ•ˆçš„åŸæ–‡ä»¶")
                        # é‡æ–°æŸ¥è¯¢æ–‡ä»¶IDä»¥ç¡®ä¿å‡†ç¡®åˆ é™¤
                        current_file_id = await self._search_file_id_by_name(event, file_name)
                        if current_file_id:
                            await self._delete_group_file(event, current_file_id, file_name)
                        else:
                            logger.warning(f"[{group_id}] æ— æ³•æŸ¥è¯¢åˆ°åŸæ–‡ä»¶IDï¼Œå¯èƒ½å·²è¢«åˆ é™¤")
            except Exception as send_e:
                logger.error(f"[{group_id}] [é˜¶æ®µä¸€] å›å¤å¤±æ•ˆé€šçŸ¥æ—¶å†æ¬¡å‘ç”Ÿé”™è¯¯: {send_e}")

    async def _check_validity_via_gfs(self, event: AstrMessageEvent, file_id: str) -> bool:
        group_id = int(event.get_group_id())
        try:
            assert isinstance(event, AiocqhttpMessageEvent)
            client = event.bot
            url_result = await client.api.call_action('get_group_file_url', group_id=group_id, file_id=file_id)
            return bool(url_result and url_result.get('url'))
        except Exception:
            return False

    def _get_preview_from_bytes(self, content_bytes: bytes) -> tuple[str, str]:
        try:
            detection = chardet.detect(content_bytes)
            encoding = detection.get('encoding', 'utf-8') or 'utf-8'
            
            if encoding and detection['confidence'] > 0.7:
                decoded_text = content_bytes.decode(encoding, errors='ignore').strip()
                return decoded_text, encoding
            
            if encoding:
                decoded_text = content_bytes.decode(encoding, errors='ignore').strip()
                return decoded_text, f"{encoding} (ä½ç½®ä¿¡åº¦å›é€€)"
            
            return "", "æœªçŸ¥"
            
        except Exception:
            return "", "æœªçŸ¥"
            
    def _is_text_file(self, file_name: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºæ”¯æŒçš„æ–‡æœ¬æ ¼å¼"""
        file_lower = file_name.lower()
        return any(file_lower.endswith(ext) for ext in self.supported_text_formats)
    
    def _is_archive_file(self, file_name: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºæ”¯æŒçš„å‹ç¼©æ ¼å¼"""
        file_lower = file_name.lower()
        return any(file_lower.endswith(ext) for ext in self.supported_archive_formats)
    
    async def _get_preview_from_archive(self, file_path: str, file_name: str) -> tuple[str, str]:
        """é€šç”¨å‹ç¼©åŒ…é¢„è§ˆæ–¹æ³•ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
        extract_path = os.path.join(self.temp_dir, f"extract_{int(time.time())}")
        os.makedirs(extract_path, exist_ok=True)
        
        archive_type = "å‹ç¼©åŒ…"
        if file_name.lower().endswith('.zip'):
            archive_type = "ZIP"
        elif file_name.lower().endswith('.7z'):
            archive_type = "7Z"
        elif file_name.lower().endswith('.rar'):
            archive_type = "RAR"
        elif any(file_name.lower().endswith(ext) for ext in ['.tar.gz', '.tgz']):
            archive_type = "TAR.GZ"
        elif any(file_name.lower().endswith(ext) for ext in ['.tar.bz2', '.tbz2']):
            archive_type = "TAR.BZ2"
        elif any(file_name.lower().endswith(ext) for ext in ['.tar.xz', '.txz']):
            archive_type = "TAR.XZ"
        elif file_name.lower().endswith('.tar'):
            archive_type = "TAR"
        
        try:
            logger.info(f"æ­£åœ¨å°è¯•è§£å‹ {archive_type} æ–‡ä»¶ï¼ˆæ— å¯†ç ï¼‰...")
            command_no_pwd = ["7za", "x", file_path, f"-o{extract_path}", "-y"]
            process = await asyncio.create_subprocess_exec(
                *command_no_pwd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            stdout, stderr = await process.communicate()

            if process.returncode != 0:
                if self.default_zip_password:
                    logger.info(f"æ— å¯†ç è§£å‹ {archive_type} å¤±è´¥ï¼Œæ­£åœ¨å°è¯•ä½¿ç”¨é»˜è®¤å¯†ç ...")
                    command_with_pwd = ["7za", "x", file_path, f"-o{extract_path}", f"-p{self.default_zip_password}", "-y"]
                    process = await asyncio.create_subprocess_exec(
                        *command_with_pwd,
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE
                    )
                    stdout, stderr = await process.communicate()
                    
                    if process.returncode != 0:
                        error_message = stderr.decode('utf-8')
                        logger.error(f"ä½¿ç”¨é»˜è®¤å¯†ç è§£å‹ {archive_type} å¤±è´¥: {error_message}")
                        return "", f"{archive_type} è§£å‹å¤±è´¥"
                else:
                    error_message = stderr.decode('utf-8')
                    logger.error(f"ä½¿ç”¨ 7za å‘½ä»¤è§£å‹ {archive_type} å¤±è´¥ä¸”æœªè®¾ç½®é»˜è®¤å¯†ç : {error_message}")
                    return "", f"{archive_type} è§£å‹å¤±è´¥"

            all_extracted_files = []
            for root, dirs, files in os.walk(extract_path):
                for f in files:
                    full_path = os.path.join(root, f)
                    all_extracted_files.append(full_path)
            
            # æŸ¥æ‰¾æ‰€æœ‰æ”¯æŒçš„æ–‡æœ¬æ–‡ä»¶
            text_files = [f for f in all_extracted_files 
                         if any(f.lower().endswith(ext) for ext in self.supported_text_formats)]
            
            if not text_files:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–‡æœ¬æ–‡ä»¶ï¼Œè¾“å‡ºå‹ç¼©åŒ…çš„æ–‡ä»¶ç»“æ„
                if not all_extracted_files:
                    return "", f"{archive_type} ä¸ºç©ºæˆ–è§£å‹å¤±è´¥"
                
                # æ„å»ºæ–‡ä»¶ç»“æ„æ ‘
                file_structure = [f"ğŸ“¦ {archive_type} æ–‡ä»¶ç»“æ„ï¼š"]
                for f_path in sorted(all_extracted_files):
                    relative_path = os.path.relpath(f_path, extract_path)
                    try:
                        file_size = os.path.getsize(f_path)
                        # æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
                        if file_size < 1024:
                            size_str = f"{file_size} B"
                        elif file_size < 1024 * 1024:
                            size_str = f"{file_size / 1024:.2f} KB"
                        elif file_size < 1024 * 1024 * 1024:
                            size_str = f"{file_size / (1024 * 1024):.2f} MB"
                        else:
                            size_str = f"{file_size / (1024 * 1024 * 1024):.2f} GB"
                        
                        # è®¡ç®—ç¼©è¿›å±‚çº§
                        depth = relative_path.count(os.sep)
                        indent = "  " * depth
                        file_name_only = os.path.basename(relative_path)
                        file_structure.append(f"{indent}â”œâ”€ {file_name_only} ({size_str})")
                    except Exception as e:
                        logger.warning(f"è·å–æ–‡ä»¶ {relative_path} ä¿¡æ¯å¤±è´¥: {e}")
                        continue
                
                structure_text = "\n".join(file_structure)
                return structure_text, "æ–‡ä»¶ç»“æ„"
                
            # ä¼˜å…ˆçº§æ’åºï¼šREADME æ–‡ä»¶ > txt/md æ–‡ä»¶ > å…¶ä»–æ–‡æœ¬æ–‡ä»¶ > æŒ‰æ–‡ä»¶å¤§å°
            def sort_priority(file_path):
                basename = os.path.basename(file_path).lower()
                # ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šREADME æ–‡ä»¶
                if basename.startswith('readme'):
                    return (0, 0, os.path.getsize(file_path))
                # ç¬¬äºŒä¼˜å…ˆçº§ï¼štxt å’Œ md æ–‡ä»¶
                elif basename.endswith('.txt'):
                    return (1, 0, os.path.getsize(file_path))
                elif basename.endswith('.md'):
                    return (1, 1, os.path.getsize(file_path))
                # ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šå…¶ä»–æ–‡æœ¬æ–‡ä»¶ï¼ŒæŒ‰å¤§å°æ’åºï¼ˆå°æ–‡ä»¶ä¼˜å…ˆï¼‰
                else:
                    return (2, 0, os.path.getsize(file_path))
            
            text_files.sort(key=sort_priority)
            first_text_file = text_files[0]
            safe_text_name = os.path.basename(first_text_file)
            
            if re.search(r'[\\/|*<>;"\x00-\x1F\x7F]', safe_text_name):
                logger.error(f"è§£å‹å‡ºçš„æ–‡ä»¶å '{safe_text_name}' åŒ…å«éå®‰å…¨å­—ç¬¦ï¼Œè·³è¿‡é¢„è§ˆã€‚")
                return "", "è§£å‹å‡ºçš„æ–‡ä»¶åä¸å®‰å…¨"

            extracted_text_path = first_text_file  # å·²ç»æ˜¯å®Œæ•´è·¯å¾„äº†
            
            with open(extracted_text_path, 'rb') as f:
                content_bytes = f.read(self.preview_length * 4)
            
            preview_text, encoding = self._get_preview_from_bytes(content_bytes)
            extra_info = f"å·²è§£å‹ã€Œ{safe_text_name}ã€(æ ¼å¼ {encoding})"
            return preview_text, extra_info
            
        except FileNotFoundError:
            logger.error("è§£å‹å¤±è´¥ï¼šå®¹å™¨å†…æœªæ‰¾åˆ° 7za å‘½ä»¤ã€‚è¯·å®‰è£… p7zip-fullã€‚")
            return "", "æœªå®‰è£… 7za"
        except Exception as e:
            logger.error(f"å¤„ç† {archive_type} æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            return "", "æœªçŸ¥é”™è¯¯"
        finally:
            if extract_path and os.path.exists(extract_path):
                try:
                    for root, dirs, files in os.walk(extract_path, topdown=False):
                        for name in files:
                            os.remove(os.path.join(root, name))
                        for name in dirs:
                            os.rmdir(os.path.join(root, name))
                    os.rmdir(extract_path)
                    logger.info(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤¹: {extract_path}")
                except Exception as e:
                    logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤¹ {extract_path} å¤±è´¥: {e}")

    async def _get_preview_for_file(self, file_name: str, file_component: Comp.File, file_size: Optional[int] = None) -> tuple[str, str]:
        is_text = self._is_text_file(file_name)
        is_archive = self.enable_zip_preview and self._is_archive_file(file_name)
        
        if not (is_text or is_archive):
            return "", ""
        
        if is_archive and file_size is not None:
            archive_size_mb = file_size / (1024 * 1024)
            limit_mb = self.zip_extraction_size_limit_mb
            
            if limit_mb > 0 and archive_size_mb > limit_mb:
                logger.debug(f"å‹ç¼©æ–‡ä»¶å¤§å° ({archive_size_mb:.2f} MB) è¶…è¿‡é…ç½®çš„ä¸Šé™ ({limit_mb} MB)ï¼Œè·³è¿‡ä¸‹è½½å’Œè§£å‹é¢„è§ˆã€‚")
                return "", "æ–‡ä»¶è¿‡å¤§ï¼Œè·³è¿‡è§£å‹"
        
        local_file_path = None
        try:
            async with self.download_semaphore:
                local_file_path = await file_component.get_file()
            if is_text:
                with open(local_file_path, 'rb') as f:
                    content_bytes = f.read(self.preview_length * 4)
                preview_text, encoding = self._get_preview_from_bytes(content_bytes)
                extra_info = f"æ ¼å¼ä¸º {encoding}"
                return preview_text, extra_info
            if is_archive:
                return await self._get_preview_from_archive(local_file_path, file_name)
        except Exception as e:
            logger.error(f"è·å–é¢„è§ˆæ—¶ä¸‹è½½æˆ–è¯»å–æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
            return "", ""
        finally:
            if local_file_path and os.path.exists(local_file_path):
                try:
                    os.remove(local_file_path)
                except OSError as e:
                    logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {local_file_path} å¤±è´¥: {e}")
        return "", ""

    async def _get_pdf_preview(self, file_path: str) -> List[str]:
        """ä½¿ç”¨ pypdfium2 ç”Ÿæˆ PDF é¢„è§ˆå›¾"""
        image_paths = []
        try:
            pdf = pdfium.PdfDocument(file_path)
            num_pages = len(pdf)
            pages_to_render = min(num_pages, self.pdf_preview_pages)
            
            for i in range(pages_to_render):
                page = pdf[i]
                bitmap = page.render(scale=2)
                image_path = os.path.join(self.temp_dir, f"pdf_preview_{int(time.time())}_{i}.png")
                bitmap.to_pil().save(image_path)
                image_paths.append(image_path)
                page.close() # é‡Šæ”¾èµ„æº
            pdf.close() # é‡Šæ”¾èµ„æº
        except Exception as e:
            logger.error(f"ç”Ÿæˆ PDF é¢„è§ˆå¤±è´¥: {e}", exc_info=True)
        return image_paths

    async def _task_delayed_recheck(self, event: AstrMessageEvent, file_name: str, file_id: str, file_component: Comp.File, preview_text: str):
        """å»¶æ—¶å¤æ ¸ä»»åŠ¡"""
        await asyncio.sleep(self.check_delay_seconds)
        group_id = int(event.get_group_id())
        
        logger.info(f"[{group_id}] [é˜¶æ®µäºŒ] å¼€å§‹å»¶æ—¶å¤æ ¸: '{file_name}'")
        
        is_still_valid = await self._check_validity_via_gfs(event, file_id)
        
        if not is_still_valid:
            logger.error(f"âŒ [{group_id}] [é˜¶æ®µäºŒ] æ–‡ä»¶ '{file_name}' åœ¨å»¶æ—¶å¤æ ¸æ—¶ç¡®è®¤å·²å¤±æ•ˆ!")
            try:
                failure_message = f"âŒ ç» {self.check_delay_seconds} ç§’åå¤æ ¸ï¼Œæ‚¨å‘é€çš„æ–‡ä»¶ã€Œ{file_name}ã€å·²å¤±æ•ˆã€‚"
                await event.send(MessageChain([Comp.Reply(id=event.message_obj.message_id), Comp.Plain(failure_message)]))
                
                # åªæœ‰åœ¨ file_component ä¸ä¸º None æ—¶æ‰å°è¯•è¡¥æ¡£
                if file_component and self.repack_file_extensions:
                    file_ext = os.path.splitext(file_name)[1].lower().lstrip('.')
                    if file_ext in self.repack_file_extensions:
                        logger.info(f"æ–‡ä»¶åœ¨å»¶æ—¶å¤æ ¸æ—¶å¤±æ•ˆï¼Œè§¦å‘é‡æ–°æ‰“åŒ…ä»»åŠ¡ (æ–‡ä»¶ç±»å‹: {file_ext})...")
                        await self._repack_and_send_file(event, file_name, file_component)
                        # è¡¥æ¡£ååˆ é™¤å·²å¤±æ•ˆçš„åŸæ–‡ä»¶
                        logger.info(f"[{group_id}] è¡¥æ¡£å®Œæˆï¼Œåˆ é™¤å·²å¤±æ•ˆçš„åŸæ–‡ä»¶")
                        # é‡æ–°æŸ¥è¯¢æ–‡ä»¶IDä»¥ç¡®ä¿å‡†ç¡®åˆ é™¤
                        current_file_id = await self._search_file_id_by_name(event, file_name)
                        if current_file_id:
                            await self._delete_group_file(event, current_file_id, file_name)
                        else:
                            logger.warning(f"[{group_id}] æ— æ³•æŸ¥è¯¢åˆ°åŸæ–‡ä»¶IDï¼Œå¯èƒ½å·²è¢«åˆ é™¤")
                elif not file_component:
                    logger.debug(f"[{group_id}] è¯¥æ–‡ä»¶ä¸ºè¡¥æ¡£åçš„æ–‡ä»¶ï¼Œæ— æ³•å†æ¬¡è¡¥æ¡£")

            except Exception as send_e:
                logger.error(f"[{group_id}] [é˜¶æ®µäºŒ] å›å¤å¤±æ•ˆé€šçŸ¥æ—¶å†æ¬¡å‘ç”Ÿé”™è¯¯: {send_e}")
        else:
            logger.info(f"âœ… [{group_id}] [é˜¶æ®µäºŒ] æ–‡ä»¶ '{file_name}' å»¶æ—¶å¤æ ¸é€šè¿‡ï¼Œä¿æŒæ²‰é»˜ã€‚")

    async def terminate(self):
        logger.info("æ’ä»¶ [ç¾¤æ–‡ä»¶é¢„è§ˆåŠ©æ‰‹] å·²å¸è½½ã€‚")